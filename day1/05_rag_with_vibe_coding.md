
# 5. RAG 개념 및 바이브 코딩으로 적용하기

## RAG (Retrieval-Augmented Generation)란?
LLM(Large Language Model, 대규모 언어모델, 이하 LLM)은 방대한 자료의 데이터를 학습하여, 앞서 나온 문맥을 바탕으로 답변을 생성합니다. 이러한 방식은 맥락이 부족한 경우, 잘못된 정보를 생성하는 모습을 보여줍니다. 이를 인공지능 학자들을 환각현상(Hallucination, 이하 할루시네이션)이라고 지칭합니다. RAG는 이러한 할루시네이션을 극복하기 위해, 언어모델에게 정답지를 미리 제공하기 위한 기술입니다. 이를 아래와 같이 간단히 정리해볼 수 있습니다.
- RAG는 **검색(Retrieval)** 과 **생성(Generation)** 을 결합한 방식으로, 외부 데이터베이스나 문서에서 관련된 정보를 찾아내고, 이 정보를 바탕으로 LLM이 답변을 생성하게 만드는 방법론입니다.

## RAG의 작동 방식
1. **질문:** 사용자가 질문을 합니다.
2. **검색:** 질문과 가장 관련 높은 문서를 외부 데이터 소스(예: PDF, Notion, DB)에서 검색합니다.
3. **정보 보강:** 검색된 문서 내용을 질문과 함께 프롬프트에 포함시킵니다.
4. **답변 생성:** LLM이 보강된 정보를 바탕으로 정확하고 근거 있는 답변을 생성합니다.

## RAG의 개념 바로알기
- 복잡한 라이브러리 없이, 프롬프트 엔지니어링만으로 RAG의 핵심을 흉내 내 볼 수 있습니다.
- **실습 목표:** 특정 텍스트를 주고, 그 안에서만 질문에 답하는 AI 만들기

### 예시 프롬프트
```
너는 주어진 문서의 내용을 바탕으로만 답변하는 AI 비서야.
아래 "문서 내용"을 참고해서 내 질문에 답해줘. 만약 문서에 없는 내용이면 "모르겠습니다"라고 답해줘.

---
[문서 내용]
이화여자대학교 AI 에이전트 개발 과정은 8월 19일부터 21일까지 3일간 진행됩니다.
강의는 n8n을 활용한 자동화, Streamlit을 이용한 웹 배포, 미니 해커톤으로 구성됩니다.
특강 연사로는 모두의연구소 김승일 소장님과 장혜정 팀장님이 참여합니다.
---

[질문]
이 과정은 며칠 동안 진행되나요?
```

---

## **바이브 코딩으로 RAG 구현하기: AI에게 무엇을 물어봐야 할까?**

'바이브 코딩'의 장점은 복잡한 시스템을 각 기능(모듈)의 조립으로 접근할 수 있다는 점입니다. RAG 시스템은 크게 **(1) 데이터를 준비하고 검색 가능하게 만드는 '준비 단계'** 와 **(2) 사용자 질문에 맞는 답을 찾아 생성하는 '실행 단계'** 로 나뉩니다.

AI 코딩 파트너에게 아래 기능들을 순서대로 요청하며 조립해 나가면 됩니다.
### **0단계: 전체 과정 준비하기(AI를 통한 계획세우기)**   
    * `Langchain을 써서 RAG 기능을 구현해보고 싶은데, 계획 세우는 것을 도와줘`    
이 요청 만으로도 AI는 상당히 많은 정보를 제공합니다. 아래 단계들은 위 요청으로 부터 이어지는 단계로 생각해볼 수 있으며, 앞으로 하는 어떤 작업이던지 이러한 방법으로 해결할 수 있는 것이 아주 많습니다. 하고자 하는 작업을 구체적으로 명시할 수록 계획도 잘 받아볼 수 있으며, 구체적으로 모른다면, 그것마저 AI와 대화하며 알아갈 수 있습니다.


### **1단계: 내 데이터를 AI가 참고할 책으로 만들기 (준비 및 색인화)**

목표: 내가 가진 문서(PDF, TXT, 웹페이지 등)를 AI가 쉽게 검색할 수 있는 '벡터' 형태로 변환하여 '벡터 DB'라는 특수 데이터베이스에 저장하는 과정입니다.

#### **1. 문서 로드하기 (Document Loader)**

* **알아야 할 개념:** RAG의 재료가 될 원본 문서(PDF, 웹사이트, 노션 등)를 프로그램 안으로 가져오는 기능입니다. 어떤 종류의 문서인지에 따라 사용하는 도구가 달라집니다.
* **🤖 AI에게 이렇게 물어보세요.**
    * `"파이썬으로 폴더 안에 있는 모든 PDF 파일의 텍스트를 읽어오는 코드 만들어줘. LangChain의 PyPDFLoader를 사용해줘."`
    * `"웹사이트 주소(URL)를 입력하면 해당 페이지의 모든 텍스트를 긁어오는 코드 짜줘. BeautifulSoup 라이브러리 써서."`

#### **2. 문서 분할하기 (Text Splitter / Chunking)**

* **알아야 할 개념:** 통째로 문서를 넣으면 너무 커서 AI가 처리하기 어렵습니다. 그래서 의미 있는 단위(보통 1,000자 내외)로 잘게 쪼개는 과정이 필요합니다. 이걸 '청킹(Chunking)'이라고 합니다.
* **🤖 AI에게 이렇게 물어보세요.**
    * `"LangChain의 RecursiveCharacterTextSplitter를 사용해서, 불러온 텍스트를 1000자 단위로 자르는 코드 보여줘. 문맥 유지를 위해 100자 정도는 겹치게(overlap) 설정해줘."`

#### **3. 텍스트를 벡터로 변환하기 (Embedding)**

* **알아야 할 개념:** RAG의 핵심 기술입니다. 쪼갠 텍스트 조각들을 AI가 의미를 이해할 수 있도록 숫자의 배열, 즉 '임베딩 벡터'로 변환하는 과정입니다. 이 벡터를 통해 컴퓨터는 '사과'와 '과일'이 '자동차'보다 의미적으로 가깝다는 것을 계산할 수 있습니다.
* **🤖 AI에게 이렇게 물어보세요.**
    * `"OpenAI의 'text-embedding-3-small' 모델을 사용해서 텍스트 목록(리스트)을 임베딩 벡터로 바꾸는 파이썬 코드 좀 짜줘."`
    * `"허깅페이스(Hugging Face)의 'ko-sbert-nli' 같은 한국어 임베딩 모델을 사용하는 방법 알려줘."`

#### **4. 벡터 저장소에 저장하기 (Vector Store)**

* **알아야 할 개념:** 변환된 벡터들을 신속하게 검색할 수 있도록 저장하는 특수한 데이터베이스입니다. 간단한 테스트용으로는 내 컴퓨터에 파일로 저장하는 `FAISS`나 `ChromaDB`를 많이 사용하고, 대규모 서비스에서는 `Pinecone` 같은 클라우드 기반 서비스를 사용합니다.
* **🤖 AI에게 이렇게 물어보세요.**
    * `"방금 만든 텍스트 조각(chunks)들과 임베딩 벡터들을 FAISS 벡터 저장소에 저장하고, 나중에 다시 불러올 수 있게 파일로 저장하는 코드 만들어줘."`
    * `"ChromaDB에 데이터를 저장하고, 특정 컬렉션을 만드는 방법을 알려줘."`

---

### **2단계: 질문에 가장 정확한 답변 찾아내기 (검색 및 생성)**

목표: 사용자의 질문을 받아, 준비된 벡터 DB에서 가장 관련 있는 정보를 찾은 뒤, 이 정보를 바탕으로 LLM(ChatGPT 등)이 신뢰도 높은 답변을 생성하게 만듭니다.

#### **검색기 만들기 (Retriever)**

* **알아야 할 개념:** 사용자의 질문도 똑같이 '임베딩 벡터'로 변환한 뒤, 벡터 저장소에서 의미적으로 가장 유사한(가까운) 텍스트 조각 몇 개를 찾아오게 시키는 부품입니다.
* **🤖 AI에게 이렇게 물어보세요.**
    * `"아까 저장해둔 FAISS 벡터 저장소 파일을 불러와서, 사용자 질문이 들어오면 가장 유사한 텍스트 조각(문서) 3개를 찾아주는 retriever 객체를 만드는 코드 보여줘. LangChain 라이브러리를 활용해줘."`

### **3단계: 프롬프트 템플릿 설계하기 (Prompt Template)**

* **알아야 할 개념:** LLM에게 그냥 질문만 던지는 게 아니라, **"이 참고자료를 보고 질문에 답해!"** 라고 명확하게 지시하는 '명령문 양식'을 만드는 것입니다. 이는 LLM이 상상해서 답하는(환각) 것을 막는 가장 중요한 장치입니다.
* **🤖 AI에게 이렇게 물어보세요.**
    * `"LangChain의 PromptTemplate을 사용해서 프롬프트를 만들고 싶어. 'context'(참고 자료)와 'question'(질문)이라는 두 변수를 받을 거야. 템플릿 양식은 이렇게 만들어줘: '다음 주어진 내용을 바탕으로 질문에 답해주세요. 내용에 없는 정보는 말하지 마세요. 내용: {context} / 질문: {question}'"`

### **4단계 LLM 연결 및 답변 생성 (LLM Chain)**

* **알아야 할 개념:** 지금까지 만든 모든 부품(Retriever, Prompt Template)과 최종 답변을 생성할 LLM(e.g., GPT)을 파이프라인처럼 연결하는 마지막 단계입니다.
* **🤖 AI에게 이렇게 물어보세요.**
    * `"LangChain Expression Language(LCEL)를 사용해서 RAG 파이프라인을 만들고 싶어. (1)사용자 질문을 받아서 (2)retriever로 관련 문서를 찾고 (3)찾은 문서를 prompt template에 넣은 뒤 (4)gemini-pro-2.5 모델에 전달해서 (5)최종 답변을 생성하는 전체 과정을 코드로 구현해줘."`

## **결론: '건축가'처럼 생각하고, AI에게 '시공'을 맡기세요**

바이브 코딩으로 RAG를 구현할 때, 여러분은 모든 코드를 이해하는 '코더'가 될 필요는 없습니다. 대신, RAG 시스템이 **[로드 → 분할 → 임베딩 → 저장 → 검색 → 프롬프트 → 생성]** 이라는 흐름으로 이루어져 있다는 것을 아는 **'설계자'** 또는 **'건축가'** 가 되어야 합니다.

이 개념적 지도만 머릿속에 있다면, 각 단계에 필요한 기능들을 AI 코딩 파트너에게 정확히 요청하여 원하는 RAG 시스템을 훨씬 빠르고 효율적으로 완성할 수 있을 것입니다.